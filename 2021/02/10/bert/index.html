<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>A beginner's learning notes to BERT | Xiaolan's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">A beginner's learning notes to BERT</h1><a id="logo" href="/.">Xiaolan's Blog</a><p class="description">Summer night breeze 夏夜晚风</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/timeline/"><i class="fa fa-align-justify"> Timeline</i></a><a href="/Gallery/"><i class="fa fa-align-justify"> Gallery</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">A beginner's learning notes to BERT</h1><div class="post-meta">2021-02-10<span> | </span><span class="category"><a href="/categories/Learning/">Learning</a></span></div><a class="disqus-comment-count" data-disqus-identifier="2021/02/10/bert/" href="/2021/02/10/bert/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-BERT"><span class="toc-number">1.</span> <span class="toc-text">What is BERT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-BERT%E2%80%99s-performance-is-great"><span class="toc-number">2.</span> <span class="toc-text">Why BERT’s performance is great</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Closer-look-at-model-architecture"><span class="toc-number">3.</span> <span class="toc-text">Closer look at model architecture</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#So%E2%80%A6-What-is-Transformer"><span class="toc-number">4.</span> <span class="toc-text">So… What is Transformer?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Several-Models-before-BERT"><span class="toc-number">5.</span> <span class="toc-text">Several Models before BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Word-Embedding"><span class="toc-number">5.1.</span> <span class="toc-text">Word Embedding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ELMo-Context-Matters"><span class="toc-number">5.2.</span> <span class="toc-text">ELMo: Context Matters</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling"><span class="toc-number">5.3.</span> <span class="toc-text">OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BERT-From-Decoders-to-Encoders"><span class="toc-number">6.</span> <span class="toc-text">BERT: From Decoders to Encoders</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#BERT-training"><span class="toc-number">6.1.</span> <span class="toc-text">BERT training</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#So-how-to-mask"><span class="toc-number">6.1.1.</span> <span class="toc-text">So.. how to mask?</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Next-sentence-prediction"><span class="toc-number">6.1.2.</span> <span class="toc-text">Next sentence prediction</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#BERT-input"><span class="toc-number">6.1.3.</span> <span class="toc-text">BERT input</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#BERT-Outputs"><span class="toc-number">6.1.4.</span> <span class="toc-text">BERT Outputs</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Use-pre-trained-fine-tuned-models-for-specific-tasks"><span class="toc-number">6.2.</span> <span class="toc-text">Use pre-trained fine-tuned models for specific tasks</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Use-BERT-for-feature-extraction"><span class="toc-number">6.3.</span> <span class="toc-text">Use BERT for feature extraction</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Next-step"><span class="toc-number">7.</span> <span class="toc-text">Next step</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reference"><span class="toc-number">8.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="post-content"><p><img src="https://lh3.googleusercontent.com/AUXkjNExXiiNUUSj-YVfHPKWNWtaeKxCM_9OnnQ6X9yt1h6nkX7ptzlam8RbR7xVZ-YZwkLD0j7-X9aC58OQZf6OtrqVLPEOj23LBZb9dkH4pDKtxtNgzBOfDqqyLjNXWrEiAf2g=w2400"></p>
<blockquote>
<p>This is my learning notes from <a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-bert/">Jay Alammar</a>.</p>
</blockquote>
<h4 id="What-is-BERT"><a href="#What-is-BERT" class="headerlink" title="What is BERT"></a>What is BERT</h4><p>BERT stands for <strong>Bidirectional Encoder Representations from Transformers</strong>. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on <em>both left and right context</em>. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. <a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/">reference</a></p>
<p><img src="https://lh3.googleusercontent.com/QubuB58pCrch4dnntdaiUyU5jLCseXIxgTtPZL_I_2gPCrIzakmrX85M-x7LZG89By0n4sk5GRsXrMHQE25BhvFYxvWHkSkZrWGmgqV7RrR-HiPdp4VWPW8lmoAU-CcLjBvj9y44=w2400"></p>
<h4 id="Why-BERT’s-performance-is-great"><a href="#Why-BERT’s-performance-is-great" class="headerlink" title="Why BERT’s performance is great"></a>Why BERT’s performance is great</h4><ol>
<li>It is bidirectional</li>
<li>It combines Mask Language Model (MLM) and Next Sentence Prediction (NSP)</li>
<li>It performs great to understand context-heavy texts</li>
</ol>
<h4 id="Closer-look-at-model-architecture"><a href="#Closer-look-at-model-architecture" class="headerlink" title="Closer look at model architecture"></a>Closer look at model architecture</h4><p>BERT is basically a trained bidirectional Transformer Encoder stack.</p>
<p><img src="https://lh3.googleusercontent.com/rTJwhnEfOATJN3aEZ93ETMMMMhW70HjoqfYsNvptCtisYOyRxBqDGPHIyH-FUZkjqEdRqwfruxYVRTOyt0vHgnf2wopWISl01msMazrBTDlWBUZnzXJvvnrylB9mD_PYzMKFTK4J=w2400"></p>
<h4 id="So…-What-is-Transformer"><a href="#So…-What-is-Transformer" class="headerlink" title="So… What is Transformer?"></a>So… What is Transformer?</h4><p>Transformer is a model that use attention to boost the training. It consists an encoding component and a decoding component, and connections between them.<br><img src="https://lh3.googleusercontent.com/MEGLErxHWQKsdhSStaBePr0U_fjiWs6iF4dbkg37moSKYlRZQpp_j9qlwUNQ52hWFCBgjgSFNmSy4LqYhCrUuHtelRMPBztIWADpvb0gwowj7RQ6Na9tijV5p6mtPrulxmyehT1Z=w2400"></p>
<p>The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word.<br><img src="https://lh3.googleusercontent.com/e9g4fIgOYZxLnmWFcIHnqaYqZ5SDjh7I-1w27JmUHNxtZ3WtcrTR_4pA8B6C2AYJLUVfmVKKlvCal5tbWD7xarN_pshtzV1GMWj32LpM8JJ9ssAxf5O8GYk4HXycEaDiL_qSjf4d=w2400"></p>
<p>This concept itself is interesting and complicated. More information can be found <a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">here</a></p>
<h4 id="Several-Models-before-BERT"><a href="#Several-Models-before-BERT" class="headerlink" title="Several Models before BERT"></a>Several Models before BERT</h4><h5 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h5><ul>
<li>What is it: use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).</li>
<li>Available in pre-trained model like Word2Vec or GloVe</li>
<li>No context information</li>
</ul>
<h5 id="ELMo-Context-Matters"><a href="#ELMo-Context-Matters" class="headerlink" title="ELMo: Context Matters"></a>ELMo: Context Matters</h5><ul>
<li>Give embedding based on the context a word is used in</li>
<li>ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings. ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called <strong>Language Modeling</strong>.</li>
<li>Use as pre-trained model</li>
</ul>
<h5 id="OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling"><a href="#OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling" class="headerlink" title="OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling"></a>OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling</h5><ul>
<li>The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.</li>
<li>Predict the next word using massive (unlabeled) datasets.</li>
<li>The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer</li>
</ul>
<h4 id="BERT-From-Decoders-to-Encoders"><a href="#BERT-From-Decoders-to-Encoders" class="headerlink" title="BERT: From Decoders to Encoders"></a>BERT: From Decoders to Encoders</h4><h5 id="BERT-training"><a href="#BERT-training" class="headerlink" title="BERT training"></a>BERT training</h5><p><img src="https://lh3.googleusercontent.com/AUXkjNExXiiNUUSj-YVfHPKWNWtaeKxCM_9OnnQ6X9yt1h6nkX7ptzlam8RbR7xVZ-YZwkLD0j7-X9aC58OQZf6OtrqVLPEOj23LBZb9dkH4pDKtxtNgzBOfDqqyLjNXWrEiAf2g=w2400"></p>
<ol>
<li>Some tokens from the input sequence are masked and the model learns to predict these words (Masked language model).</li>
<li>Two “sentences” are fed as input and the model is trained to predict if one sentence follows the other one or not (next sentence prediction NSP).</li>
<li>So we’ll feed BERT with two sentences masked, and we’ll obtain the prediction whether they’re subsequent or not, and the sentences without masked words</li>
</ol>
<h6 id="So-how-to-mask"><a href="#So-how-to-mask" class="headerlink" title="So.. how to mask?"></a>So.. how to mask?</h6><p>From each input sequence 15% of the tokens are processed as follows:</p>
<ul>
<li>with 0.8 probability the token is replaced by [MASK]</li>
<li>with 0.1 probability the token is replaced by another random token</li>
<li>with 0.1 probability the token is unchanged</li>
</ul>
<h6 id="Next-sentence-prediction"><a href="#Next-sentence-prediction" class="headerlink" title="Next sentence prediction"></a>Next sentence prediction</h6><p>These two sentences A and B are separated with the special token [SEP] and are formed in such a way that 50% of the time B is the actual next sentence and 50% of the time is a random sentence.</p>
<h6 id="BERT-input"><a href="#BERT-input" class="headerlink" title="BERT input"></a>BERT input</h6><p>The input sequence of BERT is composed by two sentences with a [SEP] token in between, and the initial “classification token” [CLS] that will later be used for prediction. Each token has a corresponding embedding, a segment embedding that identifies each sentence, and a position embedding to distinguish the position of each token (same as the positional encoding in the Transformer paper). All these embeddings are then summed up for each token.</p>
<p><img src="https://lh3.googleusercontent.com/1X9mDZDJ7qdkgU-Cid728zY48EfKDGTY--VhM47Z_BwjwJ0vOrxCCCMUazi_jsIWsqBWz--PhOB-bXDROw_ErBoYaucnTbvP9cvXgQgZfKvQQNDUcWjYZUZutxxVq0mxSz7K4NZW=w2400"></p>
<h6 id="BERT-Outputs"><a href="#BERT-Outputs" class="headerlink" title="BERT Outputs"></a>BERT Outputs</h6><p>Each position outputs a vector of size hidden_size (768 in BERT Base). For a sentence classification example , we focus on the output of only the first position (that we passed the special [CLS] token to).</p>
<p><img src="https://lh3.googleusercontent.com/F85R__AcheDjatnQnIg49Xa-S0LukksGKAuYtNlwMkQxzF6IvC_b7tAR-qyMPqkbYy9VjmdMNrt4U5Jb9Q0sfHpnb4Fx1SVFsqV2GrLEqmfVoGKfROVs0vJX6eZffZCFZbuC4o6g=w2400"></p>
<h5 id="Use-pre-trained-fine-tuned-models-for-specific-tasks"><a href="#Use-pre-trained-fine-tuned-models-for-specific-tasks" class="headerlink" title="Use pre-trained fine-tuned models for specific tasks"></a>Use pre-trained fine-tuned models for specific tasks</h5><p>BERT is a language model that can be used directly to approach other NLP tasks (summarization, question answering, etc.).<br><img src="https://lh3.googleusercontent.com/GLE92oyVawYaY3JCj-S3blEFf-9Z4h8Y7VzHnrgfG-_v3_SFRbb86ylz_ceebtaLemp189ItfML0rpC9153J-l00WsBJG0uxJf-QO9QE6F-CUhrl0TScF_VCjVLOUSF9QsloBDEY=w2400"></p>
<h5 id="Use-BERT-for-feature-extraction"><a href="#Use-BERT-for-feature-extraction" class="headerlink" title="Use BERT for feature extraction"></a>Use BERT for feature extraction</h5><p><img src="https://lh3.googleusercontent.com/BBh5hlS2yOsHfnTGXlh54Dm3ZHzvN97kuwhOVzw9ytuUcPEDowiIADh_YWcGgYf2L2-NvD5f0j7IsogIVYzM-YPP7RlkyaUr_ccCRwSSGEg2P6G47tpxlAbtnNqJBb-Xp4XlvdJc=w2400"></p>
<h4 id="Next-step"><a href="#Next-step" class="headerlink" title="Next step"></a>Next step</h4><p>Give BERT a try! I plan to take notes on this topic in my next post based on this <a target="_blank" rel="noopener" href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">post</a></p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ol>
<li><a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-bert/">http://jalammar.github.io/illustrated-bert/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/">https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/dair-ai/a-light-introduction-to-bert-2da54f96b68c">https://medium.com/dair-ai/a-light-introduction-to-bert-2da54f96b68c</a></li>
</ol>
</div><div class="tags"><a href="/tags/nlp/"><i class="fa fa-tag"></i>nlp</a></div><div class="post-nav"><a class="pre" href="/2021/02/21/sql-nosql/">SQL vs. NoSQL</a><a class="next" href="/2021/01/29/domain/">Use Custom Domain to Personal Blog with Hexo</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://watershedxiaolan.github.io/2021/02/10/bert/';
    this.page.identifier = '2021/02/10/bert/';
    this.page.title = 'A beginner's learning notes to BERT';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//watershed-1.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//watershed-1.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://watershed-1.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://watershedxiaolan.github.io"/></form></div><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="https://watershedxiaolan.github.io"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Journey/">Journey</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Learning/">Learning</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tutorial/">Tutorial</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/AB-test/" style="font-size: 25px;">AB_test</a> <a href="/tags/heroku-dash-plotly/" style="font-size: 15px;">heroku, dash, plotly</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/data/" style="font-size: 25px;">data</a> <a href="/tags/Databricks/" style="font-size: 15px;">Databricks</a> <a href="/tags/misc/" style="font-size: 15px;">misc</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/DevOps/" style="font-size: 15px;">DevOps</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/FinTech/" style="font-size: 15px;">FinTech</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/Journey/" style="font-size: 15px;">Journey</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/03/16/databricks/">Databricks Learning Notes | Part A Delta Lake Fundamentals</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/22/fintech/">Notes on Coursera's "The Future of Payment Technology"</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/21/sql-nosql/">SQL vs. NoSQL</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/10/bert/">A beginner's learning notes to BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/29/domain/">Use Custom Domain to Personal Blog with Hexo</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/24/git/">GIT Commands and Scenarios</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/13/devops/">CI/CD Pipeline with DevOps</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/13/data-storage/">Database vs. Data warehouse vs. Data lake</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/12/conda-brew-pip/">Conda vs. Homebrew vs. pip</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/10/Deploy-Dash-App-on-Heroku/">Deploy Dash App on Heroku</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//watershed-1.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/WatershedXiaolan" title="GitHub" target="_blank">GitHub</a><ul></ul><a href="https://www.linkedin.com/in/xiaolancarolli/" title="LinkedIn" target="_blank">LinkedIn</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 | <a href="/." rel="nofollow">Xiaolan's Blog | </a>Welcome! <br><!-- 不蒜子统计 --><script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv"></span>本站总访问量<span id="busuanzi_value_site_pv"></span>次<!-- 动态粒子效果 --><script type="text/javascript" color="255,23,0" opacity="0.8" zIndex="-2" count="80" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js"></script><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>